{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPlIWvrFbA/k2cYfhrlVCyS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PedroNunes99/LokaTechAssessment/blob/main/LokaAssessment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Coding part for Loka Assessment"
      ],
      "metadata": {
        "id": "rYsPzbJZGcNw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V_QruOq53G_V"
      },
      "outputs": [],
      "source": [
        "# install necessary packages in our environment\n",
        "!pip install chromadb\n",
        "!pip install sentence-transformers\n",
        "!pip install langchain\n",
        "!pip install langchainhub"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# unzip the zip file with the dataset (if necessary)\n",
        "!unzip sagemaker_documentation.zip"
      ],
      "metadata": {
        "id": "Rv_Bkwfe3yO2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# All the necessary imports\n",
        "\n",
        "import os\n",
        "\n",
        "from langchain.document_loaders import DirectoryLoader # loads all documents in a directory\n",
        "from langchain_community.document_loaders import TextLoader # loader class to use for loading files\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter # splitting text by recursively look at characters\n",
        "from langchain.embeddings import HuggingFaceEmbeddings # huggingface sentence_transformers embedding models\n",
        "from langchain.vectorstores import Chroma # vector database library\n",
        "from langchain import HuggingFaceHub  # huggingface hub -> platform with open source models\n",
        "from langchain.chains import RetrievalQA # chain for question-answering\n"
      ],
      "metadata": {
        "id": "-8ZVKX9A4giD"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# set environment variable with necessary key to access huggingface api\n",
        "os.environ['HUGGING_FACE_HUB_API_KEY'] = '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'"
      ],
      "metadata": {
        "id": "9yBU0eQC7-2E"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sagemaker_doc_path = \"sagemaker_documentation/\"\n",
        "\n",
        "# load all .md files from dataset\n",
        "loader = DirectoryLoader(sagemaker_doc_path, glob=\"./*.md\", loader_cls=TextLoader)\n",
        "files = loader.load()\n",
        "\n",
        "# create splitter\n",
        "splitter = RecursiveCharacterTextSplitter(separators=['\\n','\\n\\n','#','##','###', \" \",\"\"],chunk_size=500, chunk_overlap=200)\n",
        "\n",
        "# split documents into chunks\n",
        "docs = splitter.split_documents(files)"
      ],
      "metadata": {
        "id": "j2ANtxa13y7L"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize pre-trained embeddings from huggingface platform\n",
        "embeddings = HuggingFaceEmbeddings()"
      ],
      "metadata": {
        "id": "I0ldm0275MWr"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create vector database using the chunks we have generated and the embeddings (optional, save it in disk)\n",
        "doc_search = Chroma.from_documents(docs, embeddings, persist_directory=\"./chroma_db\")\n",
        "\n",
        "# To load from disk, uncomment this:\n",
        "# doc_search = Chroma(persist_directory=\"./chroma_db\", embedding_function=embeddings)"
      ],
      "metadata": {
        "id": "HLXtSCL65wlC"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "repo_id = \"tiiuae/falcon-7b-instruct\"\n",
        "\n",
        "# create pre-trained llm for text-generation and question-answering from huggingface api\n",
        "llm = HuggingFaceHub(huggingfacehub_api_token = os.environ['HUGGING_FACE_HUB_API_KEY'],\n",
        "                     repo_id=repo_id, model_kwargs={'temperature': 0.2, 'max_length':1000})"
      ],
      "metadata": {
        "id": "J3PPM-1H6y5G"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# set up a question-answering system chain with a user-friendly llm and\n",
        "# document search functionality (k=3). k -> number of most relevant chunks\n",
        "retrieval_chain = RetrievalQA.from_chain_type(\n",
        "    llm,\n",
        "    chain_type='stuff',\n",
        "    return_source_documents=True,\n",
        "    retriever=doc_search.as_retriever(search_kwargs={\"k\": 3})\n",
        "  )"
      ],
      "metadata": {
        "id": "8p1z2JiZ7j73"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_sources(sources):\n",
        "\n",
        "  # get list of sources (removing duplicates)\n",
        "  source_values = list(set([doc.metadata['source'] for doc in sources]))\n",
        "  print('Sources:')\n",
        "  for source in source_values:\n",
        "    print(source)\n",
        "\n",
        "\n",
        "# process_llm_response -> prints the Question, Answer and Sources\n",
        "def process_llm_response(response):\n",
        "  query = response['query']\n",
        "  result = response['result']\n",
        "  sources = response['source_documents']\n",
        "  if '\\n\\n' in result:\n",
        "    result = result.replace('\\n\\n','')\n",
        "  if 'Helpful Answer:' in result:\n",
        "    final_result = result.split('Helpful Answer:')[1]\n",
        "\n",
        "  else:\n",
        "    final_result = 'Sorry but I do not have information for your question.'\n",
        "  print('Question: ',query)\n",
        "  print()\n",
        "  print(\"Answer: \",final_result)\n",
        "  print()\n",
        "  process_sources(sources)"
      ],
      "metadata": {
        "id": "3bB7qLiP9AXf"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Testing examples from assessment file\n",
        "\n"
      ],
      "metadata": {
        "id": "D7IoroEuCQWs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query1 = \"What is SageMaker?\"\n",
        "process_llm_response(retrieval_chain.invoke(query1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H1A9tMkr1Aun",
        "outputId": "b855c80a-1466-4e41-b172-ebc297666084"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question:  What is SageMaker?\n",
            "\n",
            "Answer:   SageMaker is a fully managed machine learning service that enables data scientists and developers to build and train machine learning models using a Jupyter notebook instance.\n",
            "\n",
            "Sources:\n",
            "sagemaker_documentation/deeplens-getting-started-launch-sagemaker.md\n",
            "sagemaker_documentation/integrating-sagemaker.md\n",
            "sagemaker_documentation/sagemaker-marketplace.md\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query2 = \"What are all AWS regions where SageMaker is available?\"\n",
        "process_llm_response(retrieval_chain.invoke(query2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "to6yNihAAwTT",
        "outputId": "1e76d6d4-580b-4888-d0af-3fb312ca00ee"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question:  What are all AWS regions where SageMaker is available?\n",
            "\n",
            "Answer:  SageMaker is available in all AWS regions. You can find a list of the available regions in the AWS Marketplace documentation.\n",
            "\n",
            "Sources:\n",
            "sagemaker_documentation/sagemaker-mkt-find-subscribe.md\n",
            "sagemaker_documentation/sagemaker-marketplace.md\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query3 = \"How to check if an endpoint is KMS encrypted?\"\n",
        "process_llm_response(retrieval_chain.invoke(query3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kj4xsyg6J2YJ",
        "outputId": "f5ea0c5f-ac82-427f-d24f-7948cc071d1b"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question:  How to check if an endpoint is KMS encrypted?\n",
            "\n",
            "Answer:  You can check if an endpoint is KMS encrypted by looking at the \"Statement\" array in the \"NetworkInterfaces\" element. If the \"Effect\" property is set to \"Allow\" and the \"Action\" property is set to \"kms:Decrypt\", then the endpoint is encrypted. If the \"Effect\" property is set to \"Deny\" or the \"Action\" property is not set, then the endpoint is not\n",
            "\n",
            "Sources:\n",
            "sagemaker_documentation/sagemaker-roles.md\n",
            "sagemaker_documentation/create-an-iam-role-sagemaker-notebook.md\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query4 = \"What are SageMaker Geospatial capabilities?\"\n",
        "process_llm_response(retrieval_chain.invoke(query4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m3WfxsMeJ5D-",
        "outputId": "651636e3-c3de-4732-f63c-a9321a4b926d"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question:  What are SageMaker Geospatial capabilities?\n",
            "\n",
            "Answer:   SageMaker Geospatial capabilities include the ability to use geospatial data in your models and inference code, as well as the ability to use geospatial data to create custom visualizations and maps.\n",
            "\n",
            "Sources:\n",
            "sagemaker_documentation/sagemaker-projects-resources.md\n",
            "sagemaker_documentation/sagemaker-projects-whatis.md\n",
            "sagemaker_documentation/sagemaker-mkt-create-algo.md\n"
          ]
        }
      ]
    }
  ]
}