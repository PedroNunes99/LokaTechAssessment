{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMqeO0DnoVRxWgpi6kY9pY+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PedroNunes99/LokaTechAssessment/blob/main/LokaAssessment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Coding part for Loka Assessment"
      ],
      "metadata": {
        "id": "rYsPzbJZGcNw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V_QruOq53G_V"
      },
      "outputs": [],
      "source": [
        "# install necessary packages in our environment\n",
        "!pip install chromadb\n",
        "!pip install sentence-transformers\n",
        "!pip install langchain\n",
        "!pip install langchainhub"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# unzip the zip file with the dataset (if necessary)\n",
        "!unzip sagemaker_documentation.zip"
      ],
      "metadata": {
        "id": "Rv_Bkwfe3yO2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# All the necessary imports\n",
        "\n",
        "import os\n",
        "\n",
        "from langchain.document_loaders import DirectoryLoader # loads all documents in a directory\n",
        "from langchain_community.document_loaders import TextLoader # loader class to use for loading files\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter # splitting text by recursively look at characters\n",
        "from langchain.embeddings import HuggingFaceEmbeddings # huggingface sentence_transformers embedding models\n",
        "from langchain.vectorstores import Chroma # vector database library\n",
        "from langchain import HuggingFaceHub  # huggingface hub -> platform with open source models\n",
        "from langchain.chains import RetrievalQA # chain for question-answering\n"
      ],
      "metadata": {
        "id": "-8ZVKX9A4giD"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# set environment variable with necessary key to access huggingface api\n",
        "os.environ['HUGGING_FACE_HUB_API_KEY'] = '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'"
      ],
      "metadata": {
        "id": "9yBU0eQC7-2E"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sagemaker_doc_path = \"sagemaker_documentation/\"\n",
        "\n",
        "# load all .md files from dataset\n",
        "loader = DirectoryLoader(sagemaker_doc_path, glob=\"./*.md\", loader_cls=TextLoader)\n",
        "files = loader.load()\n",
        "\n",
        "# create splitter\n",
        "splitter = RecursiveCharacterTextSplitter(separators=['\\n','\\n\\n','#','##','###', \" \",\"\"],chunk_size=1000, chunk_overlap=200)\n",
        "\n",
        "# split documents into chunks\n",
        "docs = splitter.split_documents(files)"
      ],
      "metadata": {
        "id": "j2ANtxa13y7L"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize pre-trained embeddings from huggingface platform\n",
        "embeddings = HuggingFaceEmbeddings()"
      ],
      "metadata": {
        "id": "I0ldm0275MWr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create vector database using the chunks we have generated and the embeddings (optional, save it in disk)\n",
        "doc_search = Chroma.from_documents(docs, embeddings, persist_directory=\"./chroma_db\")\n",
        "\n",
        "# To load from disk, uncomment this:\n",
        "# doc_search = Chroma(persist_directory=\"./chroma_db\", embedding_function=embeddings)"
      ],
      "metadata": {
        "id": "HLXtSCL65wlC"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "repo_id = \"tiiuae/falcon-7b-instruct\"\n",
        "\n",
        "# create pre-trained llm for text-generation and question-answering from huggingface api\n",
        "llm = HuggingFaceHub(huggingfacehub_api_token = os.environ['HUGGING_FACE_HUB_API_KEY'],\n",
        "                     repo_id=repo_id, model_kwargs={'temperature': 0.2, 'max_length':1000})"
      ],
      "metadata": {
        "id": "J3PPM-1H6y5G"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "retrieval_chain = RetrievalQA.from_chain_type(\n",
        "    llm,\n",
        "    chain_type='stuff',\n",
        "    return_source_documents=True,\n",
        "    retriever=doc_search.as_retriever(search_kwargs={\"k\": 3})\n",
        "  )"
      ],
      "metadata": {
        "id": "8p1z2JiZ7j73"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_sources(sources):\n",
        "\n",
        "  # get list of sources (removing duplicates)\n",
        "  source_values = list(set([doc.metadata['source'] for doc in sources]))\n",
        "  print('Sources:')\n",
        "  for source in source_values:\n",
        "    print(source)\n",
        "\n",
        "\n",
        "# process_llm_response -> prints the Question, Answer and Sources\n",
        "def process_llm_response(response):\n",
        "  query = response['query']\n",
        "  result = response['result']\n",
        "  sources = response['source_documents']\n",
        "  if '\\n\\n' in result:\n",
        "    result = result.replace('\\n\\n','')\n",
        "  if 'Helpful Answer:' in result:\n",
        "    final_result = result.split('Helpful Answer:')[1]\n",
        "\n",
        "  else:\n",
        "    final_result = 'Sorry but I do not have information for your question.'\n",
        "  print('Question: ',query)\n",
        "  print()\n",
        "  print(\"Answer: \",final_result)\n",
        "  print()\n",
        "  process_sources(sources)"
      ],
      "metadata": {
        "id": "3bB7qLiP9AXf"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Testing examples from assessment file\n",
        "\n"
      ],
      "metadata": {
        "id": "D7IoroEuCQWs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query1 = \"What is SageMaker?\"\n",
        "process_llm_response(retrieval_chain.invoke(query1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H1A9tMkr1Aun",
        "outputId": "2d790be2-a730-42e0-82e2-f75814650610"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question:  What is SageMaker?\n",
            "\n",
            "Answer:  SageMaker is a cloud\\-based machine learning platform that allows you to build, train, and deploy machine learning models. It provides a variety of tools and services to help you build and deploy machine learning models quickly and easily.SageMaker is a cloud\\-based machine learning platform that allows you to build, train, and deploy machine learning models. It provides a variety of tools and services to help you build and deploy machine learning models quickly and easily.\n",
            "\n",
            "Sources:\n",
            "sagemaker_documentation/sagemaker-marketplace.md\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query2 = \"What are all AWS regions where SageMaker is available?\"\n",
        "process_llm_response(retrieval_chain.invoke(query2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "to6yNihAAwTT",
        "outputId": "c9a6f5c7-c3a6-4fc4-d58f-f6a6fac6e452"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question:  What are all AWS regions where SageMaker is available?\n",
            "\n",
            "Answer:  SageMaker is available in all AWS regions.\n",
            "\n",
            "Sources:\n",
            "sagemaker_documentation/sagemaker-mkt-find-subscribe.md\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query3 = \"How to check if an endpoint is KMS encrypted?\"\n",
        "process_llm_response(retrieval_chain.invoke(query3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kj4xsyg6J2YJ",
        "outputId": "a56e7c21-c061-4770-c2f8-d6d90d4d5fa1"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question:  How to check if an endpoint is KMS encrypted?\n",
            "\n",
            "Answer:  To check if an endpoint is KMS encrypted, you can use the following command:```\n",
            "aws kms get-key-policy --name <name of the endpoint> --query \"KeyPolicyArn\" --output text --region <region of the endpoint>\n",
            "```This will return the ARN of the key policy associated with the endpoint. If the ARN is present, it means that the endpoint is KMS encrypted.\n",
            "\n",
            "Sources:\n",
            "sagemaker_documentation/aws-resource-sagemaker-endpointconfig.md\n",
            "sagemaker_documentation/kubernetes-sagemaker-components-install.md\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query4 = \"What are SageMaker Geospatial capabilities?\"\n",
        "process_llm_response(retrieval_chain.invoke(query4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m3WfxsMeJ5D-",
        "outputId": "2142a7ff-a36c-4dd0-bc42-3285ae055fdb"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question:  What are SageMaker Geospatial capabilities?\n",
            "\n",
            "Answer:  SageMaker Geospatial capabilities include the ability to use geospatial data to train and deploy machine learning models. You can use geospatial data to train and deploy models that can be used to analyze location\\-based data, such as predicting sales for a store in a specific location. You can also use geospatial data to train and deploy models that can be used to analyze location\\-based data, such as predicting sales for a store in a specific location.\n",
            "\n",
            "\n",
            "Sources:\n",
            "sagemaker_documentation/sagemaker-projects-whatis.md\n",
            "sagemaker_documentation/sagemaker-marketplace.md\n"
          ]
        }
      ]
    }
  ]
}